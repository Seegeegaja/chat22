import os
import random
import json
import mysql.connector
from dotenv import load_dotenv

# LangChain ê´€ë ¨
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import Document
from langchain.chains import LLMChain, StuffDocumentsChain, RetrievalQA
from langchain_core.prompts import PromptTemplate

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
faiss_dir = "faiss_products_db"

if not openai_api_key:
    raise ValueError("âŒ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")

# âœ… FAISS ë¡œë“œ í•¨ìˆ˜
def load_faiss():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    return FAISS.load_local(faiss_dir, embeddings, allow_dangerous_deserialization=True)

# âœ… ë¬¸ìì—´ ì•ˆì „ ì²˜ë¦¬
def safe_str(value):
    return str(value) if value is not None else "ì •ë³´ ì—†ìŒ"

# âœ… FAQ ë¬¸ì„œ ìƒì„±
def generate_faq_documents(n=300):
    sample_questions = [
        "ì´ ì´ˆì½œë¦¿ì€ ì–´ë””ì„œ ìƒì‚°ë˜ë‚˜ìš”?",
        "ê°€ì¥ ì¸ê¸° ìˆëŠ” ì œí’ˆì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ìœ í†µê¸°í•œì´ ê¸´ ì œí’ˆì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”?",
        "ë¹„ê±´ ì´ˆì½œë¦¿ì´ ìˆë‚˜ìš”?",
        "ë¸Œëœë“œë³„ íŠ¹ì§•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ë¬´ì„¤íƒ• ì œí’ˆì´ ìˆë‚˜ìš”?",
        "ê°€ê²©ëŒ€ê°€ ì €ë ´í•œ ë¸Œëœë“œëŠ” ì–´ë–¤ ê²ƒì¸ê°€ìš”?",
        "ë‹¤í¬ ì´ˆì½œë¦¿ì€ ì–´ë–¤ ì œí’ˆì´ ìˆë‚˜ìš”?",
        "ì„ ë¬¼ìš©ìœ¼ë¡œ ì í•©í•œ ì œí’ˆì€?",
        "ì–´ë¦°ì´ì—ê²Œ ì¶”ì²œí•  ë§Œí•œ ì´ˆì½œë¦¿ì€?"
    ]
    faq_docs = []
    for i in range(n):
        question = random.choice(sample_questions)
        answer = f"{question}ì— ëŒ€í•œ ì •ë³´ëŠ” ì œí’ˆ ìƒì„¸ ì„¤ëª…ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        content = f"â“ ì§ˆë¬¸: {question}\nğŸ“ ë‹µë³€: {answer}"
        faq_docs.append(Document(
            page_content=content,
            metadata={
                "type": "faq",
                "faq_id": i + 1,
                "category": "ì¼ë°˜"
            }
        ))
    return faq_docs

# âœ… FAQ JSON íŒŒì¼ë¡œ ì €ì¥
def save_faqs_to_json(docs, filename="faqs.json"):
    faq_json = []
    for doc in docs:
        if doc.metadata.get("type") == "faq":
            lines = doc.page_content.split('\n')
            question_line = lines[0] if len(lines) > 0 else ""
            answer_line = lines[1] if len(lines) > 1 else ""
            faq_json.append({
                "question": question_line.replace("â“ ì§ˆë¬¸: ", "").strip(),
                "answer": answer_line.replace("ğŸ“ ë‹µë³€: ", "").strip(),
                "metadata": doc.metadata
            })

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(faq_json, f, ensure_ascii=False, indent=2)
    print(f"âœ… {filename} íŒŒì¼ë¡œ FAQ ì €ì¥ ì™„ë£Œ!")

# âœ… MySQLì—ì„œ ì œí’ˆ/ë¸Œëœë“œ ì •ë³´ ë¡œë“œ
def load_products_from_mysql():
    conn = mysql.connector.connect(
        host="awseb-e-ywnmbap8x2-stack-awsebrdsdatabase-ul7iljewboaj.c0vo2ike8h6l.us-east-1.rds.amazonaws.com",
        user="chocoworld",
        password="chocoworld",
        database="chocoworld"
    )
    cursor = conn.cursor()
    documents = []

    cursor.execute("""
        SELECT 
            p.title, p.content, p.price, p.stock, p.materials, p.explamation_date,
            p.weight, p.origin, p.category, p.brand_id,
            b.title AS brand_title, b.introduction AS brand_intro
        FROM product p
        LEFT JOIN brand b ON p.brand_id = b.id
    """)
    columns = [col[0] for col in cursor.description]
    rows = cursor.fetchall()

    for row in rows:
        data = dict(zip(columns, row))
        product_text = f"""
ğŸ« [ì œí’ˆ ì •ë³´]
- ì œí’ˆëª…: {safe_str(data['title'])}
- ì¹´í…Œê³ ë¦¬: {safe_str(data['category'])}
- ì›ì‚°ì§€: {safe_str(data['origin'])}
- ê°€ê²©: {safe_str(data['price'])}ì›
- ì¬ê³ : {safe_str(data['stock'])}ê°œ
- ë¬´ê²Œ: {safe_str(data['weight'])}
- ìœ í†µê¸°í•œ: {safe_str(data['explamation_date'])}
- ì¬ë£Œ: {safe_str(data['materials'])}

ğŸ“„ ì„¤ëª…:
{safe_str(data['content'])}

ğŸ·ï¸ ë¸Œëœë“œ:
- ë¸Œëœë“œëª…: {safe_str(data['brand_title'])}
- ì†Œê°œ: {safe_str(data['brand_intro'])}
""".strip()

        documents.append(Document(
            page_content=product_text,
            metadata={
                "type": "product",
                "brand_id": data["brand_id"],
                "brand_name": data["brand_title"],
                "product_name": data["title"]
            }
        ))

    cursor.execute("""
        SELECT title, content, founded, office, representative, web_site, country, introduction, history
        FROM brand
    """)
    for row in cursor.fetchall():
        (
            title, content, founded, office, representative, website,
            country, introduction, history
        ) = row

        brand_text = f"""
ğŸ·ï¸ [ë¸Œëœë“œ ì •ë³´]
- ë¸Œëœë“œëª…: {safe_str(title)}
- êµ­ê°€: {safe_str(country)}
- ì„¤ë¦½: {safe_str(founded)}, ë³¸ì‚¬: {safe_str(office)}
- ëŒ€í‘œ ì œí’ˆ: {safe_str(representative)}
- ì›¹ì‚¬ì´íŠ¸: {safe_str(website)}

ğŸ“„ ë¸Œëœë“œ ì†Œê°œ:
{safe_str(content)}

ğŸ“œ ë¸Œëœë“œ ì—­ì‚¬:
{safe_str(history)}

ğŸ“ ìš”ì•½:
{safe_str(introduction)}
""".strip()

        documents.append(Document(
            page_content=brand_text,
            metadata={
                "type": "brand",
                "brand_name": title
            }
        ))

    conn.close()
    return documents

# âœ… FAISSì— ì €ì¥
def store_in_faiss(docs, path=faiss_dir):
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    db = FAISS.from_documents(docs, embeddings)
    db.save_local(path)
    print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ â†’ {path}/")

# âœ… ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜ (FastAPIì—ì„œ ì‚¬ìš©)
# âœ… FAISS ë¡œë“œ í•¨ìˆ˜
def load_faiss():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    return FAISS.load_local(faiss_dir, embeddings, allow_dangerous_deserialization=True)

# âœ… ì „ì²´ ë¬¸ì„œì—ì„œ ë¸Œëœë“œ ëª©ë¡ ì§ì ‘ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
def count_unique_brands(db):
    results = db.similarity_search_with_score("", k=1000)
    brand_names = set()
    for doc, _ in results:
        if doc.metadata.get("type") == "brand":
            brand = doc.metadata.get("brand_name")
            if brand:
                brand_names.add(brand.strip())
    return sorted(brand_names)

# âœ… ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜
def ask_question(question, db):
    llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key)

    # âœ… ë¸Œëœë“œ ëª©ë¡ ì§ˆë¬¸ì¼ ê²½ìš°
    if any(keyword in question for keyword in ["ë¸Œëœë“œ ì¢…ë¥˜", "ë¸Œëœë“œ ë­", "ë¸Œëœë“œë§Œ", "ë¸Œëœë“œ ë¦¬ìŠ¤íŠ¸"]):
        brand_list = count_unique_brands(db)
        if not brand_list:
            return "âš ï¸ ë“±ë¡ëœ ë¸Œëœë“œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return f"í˜„ì¬ ë“±ë¡ëœ ë¸Œëœë“œëŠ” ì´ {len(brand_list)}ê°œì…ë‹ˆë‹¤:\n- " + "\n- ".join(brand_list)

    # âœ… ì œí’ˆ ì´ë¦„ë§Œ ë¬»ëŠ” ì§ˆë¬¸
    elif "ë¸Œëœë“œ" not in question and any(keyword in question for keyword in ["ì œí’ˆëª…", "ì´ë¦„ë§Œ", "ë¦¬ìŠ¤íŠ¸", "ëª©ë¡", "ë­ ìˆì–´"]):
        retriever = db.as_retriever(search_kwargs={"k": 100})
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
ë‹¹ì‹ ì€ ì´ˆì½œë¦¿ ì œí’ˆ ì´ë¦„ì„ ì •ë¦¬í•´ì£¼ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ **ì œí’ˆëª…ë§Œ** ëª©ë¡ìœ¼ë¡œ ë³´ì—¬ ì£¼ì„¸ìš”.

ğŸ“¦ ì œí’ˆ ì •ë³´:
{context}

â“ ì§ˆë¬¸:
{question}

ğŸ§¾ ì‘ë‹µ (ì œí’ˆëª…ë§Œ):
"""
        )
    # âœ… ì¼ë°˜ ì§ˆë¬¸
    else:
        retriever = db.as_retriever(search_kwargs={"k": 15})
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
ë‹¹ì‹ ì€ ê³ ê¸‰ ì´ˆì½œë¦¿ ì „ë¬¸ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ì•„ë˜ ì œí’ˆ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ **ê°„ê²°í•˜ê³  í’ˆê²© ìˆëŠ” ë¬¸ì¥**ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.

ğŸ“¦ ì œí’ˆ ì •ë³´:
{context}

â“ ì§ˆë¬¸:
{question}

ğŸ§¾ ë‹µë³€:
"""
        )

    # ê³µí†µ QA ì²´ì¸
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name="context")
    qa = RetrievalQA(retriever=retriever, combine_documents_chain=stuff_chain)
    return qa.run(question)

# âœ… ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
if __name__ == "__main__":

    product_docs = load_products_from_mysql()
    print(product_docs)

    print("ğŸ”„ FAQ 300ê°œ ìƒì„± ì¤‘...")
    faq_docs = generate_faq_documents(300)

    print("ğŸ’¾ FAQë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
    save_faqs_to_json(faq_docs, "faqs.json")

    print("ğŸ“¦ FAISSì— ì €ì¥í•  ë¬¸ì„œ ê²°í•© ì¤‘...")
    all_docs = product_docs + faq_docs

    print("ğŸ“¥ FAISSì— ë¬¸ì„œ ì €ì¥ ì¤‘...")
    store_in_faiss(all_docs)

    print("âœ… ì „ì²´ ì‘ì—… ì™„ë£Œ! ğŸ‰")
